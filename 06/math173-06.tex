\documentclass{../homework}

\homework{6}
\date{Tuesday 3/5}

\author{}
\coauthor{}

\begin{document}
\begin{problems}
\item[P.7.26] Let \(\Uspace\) be a finite-dimensional subspace of an
  \(\FF\)--inner product space \(\Vspace\) and define
  \(d(\vec v, \Uspace)\) as in (7.4.3).  Let
  \(\vec u_1, \vec u_2, \dots, \vec u_n\) be a basis for \(\Uspace\),
  let \(\vec v \in V\), and suppose that
  \(P_\Uspace \vec v = \sum_{i=1}^n c_i \vec u_i\).
  \begin{book}
    \[
      d(\vec v, \Uspace)
      = \min_{\vec u \in \Uspace} \norm{\vec v - \vec u}
      \tag{7.4.3}
    \]
  \end{book}
  \begin{enumerate}
  \item
    \label{part:d-v-u-norm}
    Show that
    \[
      d(\vec v, \Uspace)^2
      = \norm{\vec v}^2
      - \sum_{i=1}^n c_i \inner{\vec u_i, \vec v}
    \]
    for all \(\vec v \in \Vspace\).  \textit{Hint:}
    \(\vec v - P_\Uspace \vec v \in \Uspace^\perp\).

    \begin{solution}

    \end{solution}

  \item Combine \ref{part:d-v-u-norm} and the normal equations (7.4.8)
    to obtain an \((n+1) \times (n+1)\) linear system for the unknowns
    \(c_1, c_2, \dots, c_n, d(\vec v, \Uspace)^2\).  Use Cramer's rule
    to obtain
    \[
      d(\vec v, \Uspace)^2
      = \frac{g (\vec v, \vec u_1, \vec u_2, \dots, \vec u_n)}
      {g(\vec u_1, \vec u_2, \dots, \vec u_n)},
    \]
    which expresses \(d(\vec v, U)^2\) as the quotient of two Gram
    determinants.
    \begin{book}
      \[
        \begin{bmatrix}
          \inner{\vec u_1, \vec u_1} &
          \inner{\vec u_2, \vec u_1} & \cdots &
          \inner{\vec u_n, \vec u_1} \\
          \inner{\vec u_1, \vec u_2} &
          \inner{\vec u_2, \vec u_2} & \cdots &
          \inner{\vec u_n, \vec u_2} \\
          \vdots & \vdots & \ddots & \vdots \\
          \inner{\vec u_1, \vec u_n} &
          \inner{\vec u_2, \vec u_n} & \cdots &
          \inner{\vec u_n, \vec u_n}
        \end{bmatrix}
        \begin{bmatrix}
          c_1 \\ c_2 \\ \vdots \\ c_n
        \end{bmatrix}
        =
        \begin{bmatrix}
          \inner{\vec v, \vec u_1} \\
          \inner{\vec v, \vec u_2} \\
          \vdots \\
          \inner{\vec v, \vec u_n} \\
        \end{bmatrix}
        \tag{7.4.8}
      \]
    \end{book}

    \begin{solution}

    \end{solution}

  \item Let \(\vec u, \vec v \in \Vspace\) and suppose that
    \(\vec u \ne 0\).  Show that
    \[
      d(\vec v, \lspan \set{\vec u})^2
      = \frac{g(\vec v, \vec u)}{g(\vec u)}
    \]
    and conclude that \(g(\vec v, \vec u) \ge 0\).  Deduce that
    \(\abs{\inner{\vec u, \vec v}} \le \norm{\vec u} \norm{\vec v}\),
    which is the Cauchy--Schwarz inequality.

    \begin{solution}

    \end{solution}
  \end{enumerate}

\item[P.7.27] In this problem, we approach the theory of least squares
  approximation from a different perspective.  Let
  \(\Vspace, \Wspace\) be finite-dimensional \(\FF\)--inner product
  spaces with \(\dim \Vspace \le \dim \Wspace\) and let
  \(T \in L(\Vspace, \Wspace)\).
  \begin{enumerate}
  \item Show that \(\ker T = \ker T^* T\).

    \begin{solution}

    \end{solution}

  \item Prove that if \(\dim \ran T = \dim V\), then \(T^*T\) is
    invertible.

    \begin{solution}

    \end{solution}

  \item Prove that if \(\dim \ran T = \dim V\), then
    \(P = T \Paren{T^* T}^{-1} T^* \in L(\Wspace)\) is the orthogonal
    projection onto \(\ran T\).

    \begin{solution}

    \end{solution}

  \item If \(T \in L(\Vspace, \Wspace)\) and \(\dim \ran T = \dim V\),
    prove that there exists a unique vector \(\vec x \in \Vspace\)
    such that \(\norm{T \vec x - \vec y}\) is minimized.  Show that
    this vector \(\vec x\) satisfies \(T^* T \vec x = T^* \vec y\).

    \begin{solution}

    \end{solution}
  \end{enumerate}

\item[P.7.29] Find the quadratic \(y = ax^2 + bx + c\) that minimizes
  \(\sum_{i=1}^5 \Paren{y_i - \Paren{a x_i^2 + b x_i + c}}^2\) for the
  data \((-2, 3)\), \((-1, 1)\), \((0, 1)\), \((1, 2)\), \((2, 4)\).

  \begin{solution}

  \end{solution}

\item[P.7.30] In linear regression, one is given data points
  \((x_1, y_1), (x_2, y_2), \dots, (x_m, y_m)\) and must find
  parameters \(a\) and \(b\) such that
  \(\sum_{i=1}^m (y_i - ax_i - b)^2\) is minimized.  This is the same
  problem as in Example 7.5.7.  Derive explicit formulas for \(a\) and
  \(b\) that involve the quantities
  \begin{align*}
    S_x     &= \frac 1 m \sum_{i=1}^m x_i,   &
    S_y     &= \frac 1 m \sum_{i=1}^m y_i,   \\
    S_{x^2} &= \frac 1 m \sum_{i=1}^m x_i^2, &
    S_{xy}  &= \frac 1 m \sum_{i=1}^m x_i y_i.
  \end{align*}
  \begin{bookexample}[7.5.7]
    Find a least squares line \(y = ax+b\) to model the data
    \[
      (0, 1), (1, 1), (2, 3), (3, 3), (4, 4).
    \]
    According to the preceding recipe we have
    \[
      A =
      \begin{bmatrix}
        0 & 1 \\
        1 & 1 \\
        2 & 1 \\
        3 & 1 \\
        4 & 1
      \end{bmatrix},
      \qquad
      \vec x =
      \begin{bmatrix}
        a \\ b
      \end{bmatrix},
      \qquad
      \vec y =
      \begin{bmatrix}
        1 \\ 1 \\ 3 \\ 3 \\ 4
      \end{bmatrix},
    \]
    and we solve
    \[
      \underbrace{
        \begin{bmatrix}
          30 & 10 \\
          10 & 5
        \end{bmatrix}
      }_{A^* A}
      \begin{bmatrix}
        a \\ b
      \end{bmatrix}
      =
      \underbrace{
        \begin{bmatrix}
          32 \\ 12
        \end{bmatrix}
      }_{A^* \vec y},
    \]
    for \(a = b = \frac 4 5\).  Therefore, the least squares line
    \(y = \frac 4 5 x + \frac 4 5\); see Figure 7.10.
    \begin{center}
      \begin{tikzpicture}
        \begin{axis}[
          axis lines=center, axis line style=<->,
          xlabel=\(x\),
          xlabel style=right,
          ylabel=\(y\),
          ylabel style=above,
          ]
          \addplot [domain=-1.5:4.5] {4/5 * x + 4/5}
          node [sloped, near end, above] {\(y = \frac 4 5 x + \frac 4 5\)};
          \addplot [only marks] coordinates {(0,1) (1,1) (2,3) (3,3) (4,4)};

          \draw [gray] (axis cs:0,1) -- (axis cs:0,{4/5 * 0 + 4/5});
          \draw [gray] (axis cs:1,1) -- (axis cs:1,{4/5 * 1 + 4/5});
          \draw [gray] (axis cs:2,3) -- (axis cs:2,{4/5 * 2 + 4/5});
          \draw [gray] (axis cs:3,3) -- (axis cs:3,{4/5 * 3 + 4/5});
          \draw [gray] (axis cs:4,4) -- (axis cs:4,{4/5 * 4 + 4/5});
        \end{axis}
      \end{tikzpicture}
      \bookfigure{fig:least-squares-line}{7.10}{The least squares line
        corresponding to the data in Example 7.5.7.}
    \end{center}
  \end{bookexample}

  \begin{solution}

  \end{solution}

\item[P.7.31] Let \(A \in \M_{m \times n}\) and suppose that
  \(\rank A = n\).
  \begin{enumerate}
  \item Show that \(P = A \Paren{A^* A}^{-1} A^*\) is well defined.

    \begin{solution}

    \end{solution}

  \item Show that \(P\) is Hermitian and idempotent.

    \begin{solution}

    \end{solution}

  \item Show that \(\col P = \col A\), and conclude that \(P\) is the
    orthogonal projection onto \(\col A\).

    \begin{solution}

    \end{solution}
  \end{enumerate}

\item[P.7.33] Let \(\Vspace = C_\RR [0, 1]\) and let \(P\) be the
  subspace of \(\Vspace\) consisting of all real polynomials.
  \begin{enumerate}
  \item Let \(f \in \Vspace\).  The \textit{Weierstrass Approximation
      Theorem} says that for any given \(\epsilon > 0\) there is a
    polynomial \(p_\epsilon \in P\) such that
    \(\Abs{f(t) - p_\epsilon(t)} \le \epsilon\) for all
    \(t \in [0, 1]\).  If \(f \in P^\perp\), show that the \(L^2\)
    norm of \(f\) satisfies the inequality \(\norm f \le \epsilon\)
    for every \(\epsilon > 0\); see (4.5.7).  \textit{Hint:} Consider
    \(\Norm{p_\epsilon - f}^2\).
    \begin{book}
      \[
        \norm f = \Paren{\int_a^b \abs{f(t)}^2 \dif t}^{\frac 1 2}.
        \tag{4.5.7}
      \]
    \end{book}

    \begin{solution}

    \end{solution}

  \item Show that \(P^\perp = \set 0\) and conclude that
    \(P \ne \Paren{P^\perp}^\perp\).  This does not contradict
    Corollary 7.1.9 because \(\lspan P\) is not finite dimensional.

    \begin{solution}

    \end{solution}
  \end{enumerate}

\item[P.8.1] Let \(A \in \M_8\) and suppose that \(A^5 + 2A + I = 0\).
  Is \(A\) invertible?

  \begin{solution}

  \end{solution}

\item[P.8.3] Suppose that \(A \in \M_n\) is invertible and let
  \((\lambda, \vec x)\) be an eigenpair of \(A\).  Explain why
  \(\lambda \ne 0\) and show that \(\Paren{\lambda^{-1}, \vec x}\) is
  an eigenpair of \(A^{-1}\).

  \begin{solution}

  \end{solution}
\end{problems}
\end{document}
